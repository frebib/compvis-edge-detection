\documentclass[a4paper]{article}

\usepackage[margin=2.5cm]{geometry}
\usepackage[none]{hyphenat}
\usepackage{multicol}
\usepackage{titlesec}
\usepackage{graphicx} 
\graphicspath{ {test-images/} } 

\titleformat*{\section}{\large\bfseries}

\title {\vspace{-3ex}Edge detection methods for the identification of HEp-2 cell immunofluorescence in detecting anti-nuclear antibodies}
\author {1146741 \and 1467414}

\begin{document}
\maketitle
\begin{multicols*}{2}


\section*{Abstract}
\textbf{Serological testing for anti-nuclear antibodies by immunofluorescence is yet to be standardised and remains a subjective exercise liable to high inter- and intra-laboratory variation. The use of digital imaging techniques in the evaluative process can improve objectivity by entrusting algorithms with the task of discerning edges in the fluoresced image. To address this hypothesis we investigated the efficacy of various edge detectors known to the field of computational vision research. Application of ROC analysis to the results generated quantitative feedback regarding the efficacy of each edge detection method, suggesting METHOD X to be the most concrete standard by which to detect immunofluorescence.}

\section*{Introduction}

The purpose of this experiment was to objectively assess the efficacy of different edge detectors used in immunofluorescence. The methods investigated were; simple gradient, Roberts, Sobel, first order Gaussian, Laplacian and the Laplacian of Gaussian. Application of thresholding in post- processing was used to evaluate the efficacy of each method by comparing the edge points identified with those present in the pre-edge-detected images which were provided in the laboratory environment. ROC analysis was used to determine both the sensitivity and specificity of each edge detection method. 

Edge detection looks for changes in the intensity (i.e. brightness) of an image. The methods can be divided into two general groups: those which search for changes in the first derivative (eg gradient magnitude) and those which search for zero-crossings in the second derivative. 

Convolving the original image with a mask (also called a ‘filter’) will identify the rate of change for a given pixel, and this must be done in both the x and y axes. The magnitude (or an approximation thereof) is then calculated with respect to the other pixels in the image. Commonly a noise filtering method will be applied in advance of edge detection, giving an exaggerated or more clearly weighted numerical representation of an image which is therefore said to contain less ‘noise’. Examples of noise filtering include a linear filter such as a mean filter or Gaussian filtering.

\section*{Method and Materials}

We were provided with three bitmap images of fluorescing cells, along with separate images of their manually detected edges. These were to act as our target edge-detection results.
The (original) images were first converted to grayscale in order to maximise their intensity contrast and provide us with intensity images (data matrices).This simplified the task of edge detection (i.e. identifying changes of intensity in the image) as it made the various strengths of edges more distinct. The images were then inverted to leave black cells on a white background. 

\section*{Noise reduction}

Gaussian smoothing was then applied to the grayscale images as a method of noise reduction. This is a low-pass filtering technique and so suppresses high-frequency detail (i.e. noise, but also, potentially, edges) whilst preserving the low frequency parts of the image. This served to reduce the chances that a method of edge detection would be compromised by random or non-essential details present in the images. We applied two one-dimensional Gaussian filters in succession, as opposed to a single two-dimensional filter, for reasons of computational efficiency. Using filters of greater dimensions acted to preserve the local area for averaging purposes. 

\section*{Edge detection}

The various edge detection filters were then convolved with the original images. This computed the weighted sum of pixels in the image, in both the x and y axes (directions).

\section*{Thresholding}

Finally, thresholds were determined for the processed images. This was done on a case-by-case basis owing to the idiosyncrasies of each. The application of these thresholds equated to specifying which ‘edges’ were true edges (i.e. having values above the threshold) and so should be kept, and which were spurious and could be disregarded. At this stage we considered whether human evaluation of the efficacy of thresholding would be appropriate given the circumstances (i.e. carrying out eventual ROC analysis, a quantitative technique). An alternative would be to determine thresholds mathematically, for instance by hypothesising that a percentage of each image would comprise of edges and then ensuring that thresholding brought the resulting image within this range. From an analysis of the pre-edge detected images with which we had been provided we calculated the desired edge ‘content’ of the final images (post-edge detection) to be approximately 8\% (for ‘10905 JL Edges.bmp’, fig. 1), 9\% (for ‘43590 AM Edges.bmp’, fig. 2) and 10\% (for ‘9343 AM Edges.bmp’, fig. 3), respectively. We kept these approximations in mind and carried out thresholding appropriately throughout the experiment. 

\section*{Results}

\section*{Discussion}

Issues of quantitative accuracy remain pressing regarding the imaging of bioluminescent and fluorescing systems (Shelley L. Taylor, 2015). 
The issues of how best to minimise false positives (i.e. noise) and preserve image resolution in fluorescence imaging have been explored by Dedecker et al. who are in favour of the ‘superresolution’ technique. This involves reversibly photoswitching fluorescing proteins and thereby generating a sequence of images over time. A superresolution image can then be extracted using quantitative analysis of the rate of change of fluorescence, promising an image of significantly higher resolution and with reduced noise (Peter Dedekcer, 2012) . This may have produced more favourable results in the case of IMAGE X which exhibited significant background noise and loss of resolution post-processing.
Also of interest is bioluminescence tomography which sections the (3D) image and then reconstructs it. This is reported to produce significantly more stable (i.e. more consistent) luminescence, particularly with respect to the depth of the sample being processed, helping to accurately home-in on the source of luminescence. Given that we are concerned with serological testing, this may well be an important option to explore, opening up the possibility of 3D sampling. Bioluminescence tomography ‘…therefore has the potential to both increase the amount and the accuracy of quantitative data attained by luminescence imaging’ (James A Guggenheim, 2014).

\section*{References}

James A Guggenheim, H. R. (2014). Bioluminescence tomography improves quantitative accuracy for pre-clinical imaging. International Society for Optics and Photonics, 87990G-87990G-6.
Peter Dedekcer, G. C. (2012). Widely accessible method for superresolution fluorescence imaging of living systems. Proceedings of the National Academy of Sciences of the United States of America, 10909-10914.
Shelley L. Taylor, T. A. (2015). Accounting for systematic errors in bioluminescence imaging to improve quantitative accuracy. Diffuse Optical Imaging V, 10.1117/12.2183756.

\end{multicols*}

\end{document}
